/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.cassandra.distributed.upgrade;

import java.io.File;
import java.io.IOException;
import java.nio.file.Path;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;
import java.util.concurrent.atomic.AtomicInteger;

import com.google.common.util.concurrent.Uninterruptibles;
import org.junit.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.cassandra.distributed.UpgradeableCluster;
import org.apache.cassandra.distributed.api.ConsistencyLevel;
import org.apache.cassandra.distributed.api.Feature;
import org.apache.cassandra.distributed.api.IInvokableInstance;
import org.apache.cassandra.distributed.api.IUpgradeableInstance;
import org.apache.cassandra.distributed.api.NodeToolResult;
import org.apache.cassandra.io.util.FileUtils;
import org.apache.cassandra.io.util.PathUtils;
import org.apache.cassandra.schema.CIEInternalKeyspace;
import org.apache.cassandra.service.StorageProxy;

import static org.apache.cassandra.utils.Clock.Global.currentTimeMillis;
import static org.assertj.core.api.Assertions.assertThat;
import static org.junit.Assert.assertEquals;

/* Test the replacement patch for the internal denylist patch now that it has been upstreamed.  The upstream
   patch uses a new table in system_distributed with slightly different column names.  The initial test table
   and denylist was generated on a 3.0.24 instance as it is not easy to execute lambdas on anything other than
   the current version.

   The initial test data is just NUM_TEST_KEYS of an integer primary key, where pk%DENY_MODULO==0 is denylisted.
   (pk 0, 5, 10 etc)

   1) Create a 3.0.24 cluster
   2) Restore a test table and pre-filled denylist by using nodetool import on each node and cleaning up
      incorrectly partitioned data.
   3) Upgrade the nodes one at a time. After each upgrade, blocklist all keys pk%DENY_MODULO=instanceId.  As
      more nodes are upgraded, more keys are blocked.
   4) Once the whole cluster is upgraded, remove the newly created denylists and check it is possible to read
      all the pk's except 0, 5, 10, 15.
 */
public class LegacyDenylistUpgradeTest extends UpgradeTestBase
{
    final static int NUM_NODES = 4; // One more than the RF for distributed keyspace
    final static int NUM_TEST_KEYS = 200; // NUM_TEST_KEYS and DENY_MODULO must match imported sstables
    final static int DENY_MODULO = 5; // block 1-in-5
    private final static Logger logger = LoggerFactory.getLogger(LegacyDenylistUpgradeTest.class);

    @Test
    public void migrationTest() throws Throwable
    {
        new UpgradeTestBase.TestCase()
        .nodes(NUM_NODES)
        .singleUpgrade(v30)
        .withConfig(config -> config.with(Feature.GOSSIP, Feature.NETWORK)
                                    .set("key_cache_size", "0MiB")
                                    .set("blacklist_refresh_period_seconds", 1)
                                    .set("blacklist_initial_load_retry_seconds", 1)
                                    .set("enable_partition_blacklist", "true")
                                    .set("max_blacklist_keys_per_cf", "9999")
                                    .set("reject_out_of_token_range_requests", "false")) // for initial data load
        .setup((cluster) -> {
            cluster.schemaChange(String.format("CREATE TABLE %s.tbl (pk int, PRIMARY KEY (pk));", KEYSPACE));
            cluster.schemaChange("ALTER KEYSPACE " + CIEInternalKeyspace.NAME + " WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': " + Math.min(cluster.size(), 3) + "}");

            // Load data generated by 3.0.24. Was not able to get 3.0.24 to generate as part of the upgrade
            // without messing with classpaths so it could load a lambda.  Instead the data is from a snapshot
            // of a single-node cluster that created denylist entries.  Nodetool import is node-local, so rather
            // than calculate partitioning, just load on each of the upgradeable cluster nodes and run cleanup
            // afterwards to trim incorrectly partitioned data.
            Path temporarySstableDir = FileUtils.getTempDir().toPath().resolve("legacy-denylist");
            PathUtils.tryCreateDirectories(temporarySstableDir);
            cluster.forEach(instance -> {
                try
                {
                    org.apache.commons.io.FileUtils.copyDirectory(new File("test/data/legacy-denylist"), temporarySstableDir.toFile());
                    NodeToolResult dl = instance.nodetoolResult("import", "--no-verify", "--no-tokens", "cie_internal", "partition_blacklist", temporarySstableDir.resolve("cie_internal/partition_blacklist").toString());
                    assertEquals(0, dl.getRc());
                    NodeToolResult tbl = instance.nodetoolResult("import", "--no-verify", "--no-tokens", "distributed_test_keyspace", "tbl", temporarySstableDir.resolve("distributed_test_keyspace/tbl").toString());
                    assertEquals(0, tbl.getRc());
                    NodeToolResult cleanup = instance.nodetoolResult("cleanup");
                    assertEquals(0, cleanup.getRc());
                }
                catch (IOException e)
                {
                    throw new RuntimeException(e);
                }
            });

            Object blResults[][] = cluster.coordinator(1).execute("SELECT * FROM cie_internal.partition_blacklist", ConsistencyLevel.ALL);
            assertThat(blResults).hasSize(NUM_TEST_KEYS / DENY_MODULO); // entry for every pk %DENY_MODULO(5)==0 denylisted
            Object tblResults[][] = cluster.coordinator(1).execute("SELECT * FROM distributed_test_keyspace.tbl", ConsistencyLevel.ALL);
            assertThat(tblResults).hasSize(NUM_TEST_KEYS); // test data has NUM_TEST_KEY(200)

            // Cross-version is fiddly for synchronization primitives other than the log (which is not useful in this
            // case) wait at least denylist refresh time before testing.
            Uninterruptibles.sleepUninterruptibly(5, TimeUnit.SECONDS); // self-loathing.

            validateDenylist(cluster, 0); // initial denylist with zero offset
            logger.info("Initial cluster setup denylist validated");
        })
        .runAfterNodeUpgrade((cluster, instanceId) -> {
            logger.info("Node {} upgraded", instanceId);

            // On the new instance, create some new denylist entries.
            // The cast is unpleasant, but safe to do so as the upgraded instance is running the current version.
            ((IInvokableInstance) cluster.get(instanceId)).runOnInstance(() -> {
                for (int pkToDenylist = instanceId; pkToDenylist < NUM_TEST_KEYS; pkToDenylist += DENY_MODULO)
                {
                    if (instanceId % 2 == 0)
                    {
                        StorageProxy.instance.blacklistKey(KEYSPACE, "tbl", Integer.toString(pkToDenylist));
                    }
                    else
                    {
                        StorageProxy.instance.denylistKey(KEYSPACE, "tbl", Integer.toString(pkToDenylist));
                    }
                }
            });

            // Cross-version is fiddly for synchronization
            Uninterruptibles.sleepUninterruptibly(5, TimeUnit.SECONDS); // self-loathing.

            // Check with the updated denylist in place - pk%DENY_MODULO <= instanceId now blacklisted
            // (relies on instances being upgraded in instanceId order)
            validateDenylist(cluster, instanceId);
        })
        .runAfterClusterUpgrade((cluster) -> {
            logger.info("Upgrades complete");

            // Check that the legacy and denylist tables have the expected number of keys.
            // Only instance4 will have anything written to the denylist table as before that it was a mixed-major
            // version cluster.
            Object dl[][] = cluster.coordinator(4).execute("SELECT * FROM system_distributed.partition_denylist;", ConsistencyLevel.ALL);
            Object bl[][] = cluster.coordinator(4).execute("SELECT * FROM cie_internal.partition_blacklist;", ConsistencyLevel.ALL);
            assertThat(dl).hasSize(40); // from the pk%DENY_MODULO=4 entries added
            assertThat(bl).hasSize(200); // everything should now be blocked

            // Now everything is upgraded, unblocklist everything except pk%DENY_MODULO
            // Have had to call removeDenylist on each node to get working - without there are some pk's from
            // the instanceId 4 upgrade still in the OSS denylist and are not removed.
            // rdar://89149173 (4.0: Investigate why removeDenylist needed to be called on all instances in the upgrade test)
            for (int pk = 0; pk < NUM_TEST_KEYS; pk ++)
            {
                final int pkToUndenylist = pk;
                if (pkToUndenylist % DENY_MODULO != 0)
                {
                    cluster.forEach(upgradeableInstance -> {
                        IInvokableInstance instance = ((IInvokableInstance) upgradeableInstance);
                        instance.runOnInstance(() ->
                                               {
                                                   StorageProxy.instance.removeDenylistKey(KEYSPACE, "tbl", Integer.toString(pkToUndenylist));
                                               });
                    });
                }
            }

            dl = cluster.coordinator(4).execute("SELECT * FROM system_distributed.partition_denylist;", ConsistencyLevel.QUORUM);
            bl = cluster.coordinator(4).execute("SELECT * FROM cie_internal.partition_blacklist;", ConsistencyLevel.QUORUM);
            assertThat(dl).hasSize(0);  // all the denylist entries from instanceId4 should have been removed
            assertThat(bl).hasSize(40); // only the original entries from pk%DENY_MODULO==0 should remains

            // Cross-version is fiddly for synchronization
            Uninterruptibles.sleepUninterruptibly(5, TimeUnit.SECONDS); // self-loathing.
            validateDenylist(cluster, 0);

            // Bounce all the nodes one last time to allow the migration to happen
            // now there are no 3.0 nodes left
            cluster.forEach(instance -> {
                try
                {
                    instance.shutdown(true).get(1, TimeUnit.MINUTES);
                    instance.startup();
                }
                catch (Throwable e)
                {
                    throw new RuntimeException(e);
                }
            });

            logger.info("Dropping legacy denylist table");
            cluster.schemaChange("DROP TABLE cie_internal.partition_blacklist");

            // Cross-version is fiddly for synchronization
            Uninterruptibles.sleepUninterruptibly(5, TimeUnit.SECONDS); // self-loathing.
            validateDenylist(cluster, 0);
        })
        .run();
    }

    static void validateDenylist(UpgradeableCluster cluster, final int denylistedOffset)
    {
        String selectPK = String.format("SELECT pk FROM %s.tbl WHERE pk = ?;", KEYSPACE);
        String insertPK = String.format("INSERT INTO %s.tbl(pk) VALUES (?);", KEYSPACE);

        cluster.forEach(instance -> {
            int instanceId = instance.config().num();
            logger.info("Validating denylist with pk%{} <= {} denylisted on instanceId {}", DENY_MODULO, denylistedOffset, instanceId);

            // Query to trigger the cache refresh in the background
            instance.coordinator().execute(selectPK, ConsistencyLevel.ALL, -1);

            for (int pk = 0; pk < NUM_TEST_KEYS; pk++)
            {
                boolean expectDenyListed = (pk % DENY_MODULO) <= denylistedOffset;
                Object[][] result = null;
                int attempts = 0;
                final int RETRY_LIMIT = 100;
                Exception lastEx;
                boolean wasDenyListed;
                long startTime = currentTimeMillis();
                long now;
                do
                {
                    // Seem flaky - give it a few goes around to rebuild the cache if needed
                    try
                    {
                        result = instance.coordinator().execute(selectPK, ConsistencyLevel.ALL, pk);
                        lastEx = null;
                        wasDenyListed = false;
                    }
                    catch (Exception ex)
                    {
                        lastEx = ex;
                        wasDenyListed = true;
                    }
                    attempts++;
                    now = currentTimeMillis();
                } while (wasDenyListed != expectDenyListed && now - startTime < 15000);

                if (attempts > 1)
                    logger.info("pk {} for InstanceId{} took {} attempts in {} millis", pk, instanceId, attempts, now - startTime);
                if (wasDenyListed != expectDenyListed) // rerun in case in the debugger and want to try and trace
                    instance.coordinator().execute(selectPK, ConsistencyLevel.ALL, pk);

                assertThat(wasDenyListed).isEqualTo(expectDenyListed);

                if (expectDenyListed)
                {
                    assertThat(lastEx).isNotNull();
                    assertThat(lastEx.getClass().getCanonicalName()).isEqualTo("org.apache.cassandra.exceptions.InvalidRequestException");
                }
                else
                {
                    assertThat(result).as("result not null for pk %d instance %d", pk, instanceId).isNotNull();
                    assertThat(result).as("result size for pk %d instance %d", pk, instanceId).hasSize(1);
                    assertThat(result[0][0]).as("result for pk %d instance %d", pk, instanceId).isEqualTo(pk);
                }
            }
        });
    }


    /* Copy the methods below to a new test on a 3.0.24 instance to generate the test data

    void snapshotAndCopy(ColumnFamilyStore cfs, File base)
    {
        Set<SSTableReader> sstables = cfs.snapshot("legacy-denylist");

        File destination = base.toPath().resolve(cfs.keyspace.getName()).resolve(cfs.name).toFile();
        destination.mkdirs();

        sstables.forEach(sst -> {
            for (String filename : sst.getAllFilePaths())
            {
                try
                {
                    org.apache.commons.io.FileUtils.copyFile(new File(filename), destination);
                }
                catch (IOException ex)
                {
                    throw new RuntimeException(ex);
                }
            }
        });
    }

    @Test
    public void generateData() throws Throwable
    {
        try (Cluster cluster = init(Cluster.create(1)))
        {
            fixDistributedSchemas(cluster);
            cluster.schemaChange("ALTER KEYSPACE " + CIEInternalKeyspace.NAME + " WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': " + Math.min(cluster.size(), 3) + "}");

            cluster.schemaChange(String.format("CREATE TABLE %s.tbl (pk int, PRIMARY KEY (pk));", KEYSPACE));
            String insertQuery = String.format("INSERT INTO %s.tbl(pk) VALUES (?);", KEYSPACE);

            IInvokableInstance instance = cluster.get(1);
            for (int pk = 0; pk < NUM_TEST_KEYS; pk++)
            {
                instance.executeInternal(insertQuery, pk);
                if (pk % DENY_MODULO == 0)
                {
                    final int pkToDenylist = pk;
                    instance.runOnInstance(() -> {
                        StorageProxy.instance.blacklistKey(KEYSPACE, "tbl", Integer.toString(pkToDenylist));
                    });
                }

            }

            // Copy the sstables to test/data/legacy-denylist
            File base = new File("test/data/legacy-denylist");

            instance.runOnInstance(() -> {
                ColumnFamilyStore tblCF = ColumnFamilyStore.getIfExists(KEYSPACE, "tbl");
                //snapshotAndCopy(tblCF, destination);
                Set<SSTableReader> tblSSTs = tblCF.snapshot("legacy-denylist");
                File tblDestination = base.toPath().resolve(tblCF.keyspace.getName()).resolve(tblCF.name).toFile();
                tblDestination.mkdirs();

                tblSSTs.forEach(sst -> {
                    for (String filename : sst.getAllFilePaths())
                    {
                        try
                        {
                            org.apache.commons.io.FileUtils.copyFileToDirectory(new File(filename), tblDestination);
                        }
                        catch (IOException ex)
                        {
                            throw new RuntimeException(ex);
                        }
                    }
                });

                ColumnFamilyStore denylistCF = ColumnFamilyStore.getIfExists(CIEInternalKeyspace.NAME, PartitionBlacklist.PARTITION_BLACKLIST_CF);
                Set<SSTableReader> dlSSTs = denylistCF.snapshot("legacy-denylist");
                File dlDestination = base.toPath().resolve(denylistCF.keyspace.getName()).resolve(denylistCF.name).toFile();
                dlDestination.mkdirs();

                dlSSTs.forEach(sst -> {
                    for (String filename : sst.getAllFilePaths())
                    {
                        try
                        {
                            org.apache.commons.io.FileUtils.copyFileToDirectory(new File(filename), dlDestination);
                        }
                        catch (IOException ex)
                        {
                            throw new RuntimeException(ex);
                        }
                    }
                });
            });
        }
    }
    */
}
